{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM+CNN MODEL FOR CLASSIFICATION.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnParken/Deep-Learning-with-PyTorch-Tutorials/blob/master/LSTM%2BCNN_MODEL_FOR_CLASSIFICATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WueQ8HT9sf4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "dc971ab4-1c45-47dd-82f4-4798dcaca6ac"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky-V6HdGntVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import csv, re, random, sys, os, time, datetime\n",
        "import numpy as np\n",
        "from IPython import embed\n",
        "from tensorflow.contrib import learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEsyX9Kv-PzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/TF1.4/data/Sentiment Analysis Dataset\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-dPSabgnkvU",
        "colab_type": "text"
      },
      "source": [
        "### 数据预处理部分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnnbEuDTnZSA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "143fafed-2302-4c3f-fd0a-b8577cb15bc5"
      },
      "source": [
        "\n",
        "#Separates a file with mixed positive and negative examples into two.\n",
        "def separate_dataset(filename):\n",
        "    good_out = open(\"good_\"+filename,\"w+\");\n",
        "    bad_out  = open(\"bad_\"+filename,\"w+\");\n",
        "\n",
        "    seen = 1;\n",
        "    with open(filename,'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        # reader.next()   python2.x\n",
        "        next(reader)\n",
        "\n",
        "        for line in reader:\n",
        "            seen +=1\n",
        "            sentiment = line[1]\n",
        "            sentence = line[3]\n",
        "\n",
        "            if (sentiment == \"0\"):\n",
        "                bad_out.write(sentence+\"\\n\")\n",
        "            else:\n",
        "                good_out.write(sentence+\"\\n\")\n",
        "\n",
        "            if (seen%10000==0):\n",
        "                print (seen);\n",
        "\n",
        "    good_out.close();\n",
        "    bad_out.close();\n",
        "\n",
        "#Load Dataset\n",
        "def get_dataset(goodfile,badfile,limit,randomize=True):\n",
        "    good_x = list(open(goodfile,\"r\").readlines())\n",
        "    good_x = [s.strip() for s in good_x]\n",
        "    \n",
        "    bad_x  = list(open(badfile,\"r\").readlines())\n",
        "    bad_x  = [s.strip() for s in bad_x]\n",
        "\n",
        "    if (randomize):\n",
        "        random.shuffle(bad_x)\n",
        "        random.shuffle(good_x)\n",
        "\n",
        "    good_x = good_x[:limit]\n",
        "    bad_x = bad_x[:limit]\n",
        "\n",
        "    x = good_x + bad_x\n",
        "    x = [clean_str(s) for s in x]\n",
        "\n",
        "\n",
        "    positive_labels = [[0, 1] for _ in good_x]\n",
        "    negative_labels = [[1, 0] for _ in bad_x]\n",
        "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
        "    return [x,y]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Clean Dataset\n",
        "def clean_str(string):\n",
        "\n",
        "\n",
        "    #EMOJIS\n",
        "    string = re.sub(r\":\\)\",\"emojihappy1\",string)\n",
        "    string = re.sub(r\":P\",\"emojihappy2\",string)\n",
        "    string = re.sub(r\":p\",\"emojihappy3\",string)\n",
        "    string = re.sub(r\":>\",\"emojihappy4\",string)\n",
        "    string = re.sub(r\":3\",\"emojihappy5\",string)\n",
        "    string = re.sub(r\":D\",\"emojihappy6\",string)\n",
        "    string = re.sub(r\" XD \",\"emojihappy7\",string)\n",
        "    string = re.sub(r\" <3 \",\"emojihappy8\",string)\n",
        "\n",
        "    string = re.sub(r\":\\(\",\"emojisad9\",string)\n",
        "    string = re.sub(r\":<\",\"emojisad10\",string)\n",
        "    string = re.sub(r\":<\",\"emojisad11\",string)\n",
        "    string = re.sub(r\">:\\(\",\"emojisad12\",string)\n",
        "\n",
        "    #MENTIONS \"(@)\\w+\"\n",
        "    string = re.sub(r\"(@)\\w+\",\"mentiontoken\",string)\n",
        "    \n",
        "    #WEBSITES\n",
        "    string = re.sub(r\"http(s)*:(\\S)*\",\"linktoken\",string)\n",
        "\n",
        "    #STRANGE UNICODE \\x...\n",
        "    string = re.sub(r\"\\\\x(\\S)*\",\"\",string)\n",
        "\n",
        "    #General Cleanup and Symbols\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "\n",
        "    return string.strip().lower()\n",
        "\n",
        "\n",
        "\n",
        "#Generate random batches\n",
        "#Source: https://github.com/dennybritz/cnn-text-classification-tf/blob/master/data_helpers.py\n",
        "def gen_batch(data, batch_size, num_epochs, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generates a batch iterator for a dataset.\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    data_size = len(data)\n",
        "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data at each epoch\n",
        "        if shuffle:\n",
        "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "            shuffled_data = data[shuffle_indices]\n",
        "        else:\n",
        "            shuffled_data = data\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "            yield shuffled_data[start_index:end_index]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    separate_dataset(\"Sentiment Analysis Dataset.csv\");\n",
        "\n",
        "#42\n",
        "#642"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "100000\n",
            "110000\n",
            "120000\n",
            "130000\n",
            "140000\n",
            "150000\n",
            "160000\n",
            "170000\n",
            "180000\n",
            "190000\n",
            "200000\n",
            "210000\n",
            "220000\n",
            "230000\n",
            "240000\n",
            "250000\n",
            "260000\n",
            "270000\n",
            "280000\n",
            "290000\n",
            "300000\n",
            "310000\n",
            "320000\n",
            "330000\n",
            "340000\n",
            "350000\n",
            "360000\n",
            "370000\n",
            "380000\n",
            "390000\n",
            "400000\n",
            "410000\n",
            "420000\n",
            "430000\n",
            "440000\n",
            "450000\n",
            "460000\n",
            "470000\n",
            "480000\n",
            "490000\n",
            "500000\n",
            "510000\n",
            "520000\n",
            "530000\n",
            "540000\n",
            "550000\n",
            "560000\n",
            "570000\n",
            "580000\n",
            "590000\n",
            "600000\n",
            "610000\n",
            "620000\n",
            "630000\n",
            "640000\n",
            "650000\n",
            "660000\n",
            "670000\n",
            "680000\n",
            "690000\n",
            "700000\n",
            "710000\n",
            "720000\n",
            "730000\n",
            "740000\n",
            "750000\n",
            "760000\n",
            "770000\n",
            "780000\n",
            "790000\n",
            "800000\n",
            "810000\n",
            "820000\n",
            "830000\n",
            "840000\n",
            "850000\n",
            "860000\n",
            "870000\n",
            "880000\n",
            "890000\n",
            "900000\n",
            "910000\n",
            "920000\n",
            "930000\n",
            "940000\n",
            "950000\n",
            "960000\n",
            "970000\n",
            "980000\n",
            "990000\n",
            "1000000\n",
            "1010000\n",
            "1020000\n",
            "1030000\n",
            "1040000\n",
            "1050000\n",
            "1060000\n",
            "1070000\n",
            "1080000\n",
            "1090000\n",
            "1100000\n",
            "1110000\n",
            "1120000\n",
            "1130000\n",
            "1140000\n",
            "1150000\n",
            "1160000\n",
            "1170000\n",
            "1180000\n",
            "1190000\n",
            "1200000\n",
            "1210000\n",
            "1220000\n",
            "1230000\n",
            "1240000\n",
            "1250000\n",
            "1260000\n",
            "1270000\n",
            "1280000\n",
            "1290000\n",
            "1300000\n",
            "1310000\n",
            "1320000\n",
            "1330000\n",
            "1340000\n",
            "1350000\n",
            "1360000\n",
            "1370000\n",
            "1380000\n",
            "1390000\n",
            "1400000\n",
            "1410000\n",
            "1420000\n",
            "1430000\n",
            "1440000\n",
            "1450000\n",
            "1460000\n",
            "1470000\n",
            "1480000\n",
            "1490000\n",
            "1500000\n",
            "1510000\n",
            "1520000\n",
            "1530000\n",
            "1540000\n",
            "1550000\n",
            "1560000\n",
            "1570000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRYPBnD2o5YJ",
        "colab_type": "text"
      },
      "source": [
        "### LSTM+CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjobduAQo39-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://blog.csdn.net/zjrn1027/article/details/80090749\n",
        "class LSTM_CNN(object):\n",
        "    def __init__(self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0, num_hidden=100):\n",
        "\n",
        "\n",
        "        # PLACEHOLDERS\n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")    # X - The Data\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")      # Y - The Lables\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")       # Dropout\n",
        "\n",
        "        \n",
        "        l2_loss = tf.constant(0.0) # Keeping track of l2 regularization loss\n",
        "\n",
        "\n",
        "        #1. EMBEDDING LAYER ################################################################\n",
        "        \n",
        "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
        "            self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),name=\"W\")\n",
        "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
        "            #self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "\n",
        "\n",
        "        #2. LSTM LAYER ######################################################################\n",
        "\n",
        "        # https://blog.csdn.net/u013230189/article/details/82811066\n",
        "        # https://blog.csdn.net/wjc1182511338/article/details/79689409\n",
        "\n",
        "        self.lstm_cell = tf.contrib.rnn.LSTMCell(embedding_size,state_is_tuple=True) # embedding_size > conv2d\n",
        "        #self.h_drop_exp = tf.expand_dims(self.h_drop,-1)\n",
        "        # https://www.cnblogs.com/lovychen/p/9294624.html\n",
        "        self.lstm_out,self.lstm_state = tf.nn.dynamic_rnn(self.lstm_cell,self.embedded_chars,dtype=tf.float32) # [batch_size, max_time, cell.output_size]\n",
        "        #embed()\n",
        "                                      # [batch, in_height, in_width, in_channels]\n",
        "        self.lstm_out_expanded = tf.expand_dims(self.lstm_out, -1) # [batch_size, max_time, cell.output_size, 1]\n",
        "\n",
        "\n",
        "        #2. CONVOLUTION LAYER + MAXPOOLING LAYER (per filter) ###############################\n",
        "\n",
        "        pooled_outputs = []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                # CONVOLUTION LAYER\n",
        "\n",
        "                # https://blog.csdn.net/mao_xiao_feng/article/details/78004522\n",
        "                # https://blog.csdn.net/mao_xiao_feng/article/details/53444333\n",
        "                filter_shape = [filter_size, embedding_size, 1, num_filters] # [filter_height, filter_width, in_channels, out_channels]\n",
        "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
        "                conv = tf.nn.conv2d(self.lstm_out_expanded, W,strides=[1, 1, 1, 1],padding=\"VALID\",name=\"conv\")\n",
        "                # NON-LINEARITY\n",
        "\n",
        "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
        "                # MAXPOOLING\n",
        "\n",
        "                pooled = tf.nn.max_pool(h, ksize=[1, sequence_length - filter_size + 1, 1, 1], strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\n",
        "                pooled_outputs.append(pooled)\n",
        "\n",
        "\n",
        "        # COMBINING POOLED FEATURES\n",
        "\n",
        "        # https://blog.csdn.net/leviopku/article/details/82380118\n",
        "        num_filters_total = num_filters * len(filter_sizes)\n",
        "        self.h_pool = tf.concat(pooled_outputs, 3) # [[batch_size, height, width, channals], [...], [...]]\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "\n",
        "\n",
        "        # #3. DROPOUT LAYER ###################################################################\n",
        "\n",
        "        # https://blog.csdn.net/huahuazhu/article/details/73649389\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
        "\n",
        "\n",
        "        # Final (unnormalized) scores and predictions\n",
        "\n",
        "        # https://blog.csdn.net/jerr__y/article/details/60877873\n",
        "        # https://www.w3cschool.cn/tensorflow_python/tf_nn_xw_plus_b.html\n",
        "        \n",
        "        with tf.name_scope(\"output\"):\n",
        "            W = tf.get_variable(\n",
        "                \"W\",\n",
        "                shape=[num_filters_total, num_classes],\n",
        "                initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
        "            # https://blog.csdn.net/yangfengling1023/article/details/82910536\n",
        "            l2_loss += tf.nn.l2_loss(W)\n",
        "            l2_loss += tf.nn.l2_loss(b)\n",
        "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
        "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "\n",
        "\n",
        "        # CalculateMean cross-entropy loss\n",
        "        # https://blog.csdn.net/mao_xiao_feng/article/details/53382790\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y) # 这是一个向量\n",
        "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "\n",
        "        # Accuracy\n",
        "        # https://blog.csdn.net/zhuzuwei/article/details/78983562\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
        "\n",
        "\n",
        "        print (\"(!!) LOADED LSTM-CNN! :)\")\n",
        "        #embed()\n",
        "\n",
        "# 1. Embed --> LSTM\n",
        "# 2. LSTM --> CNN\n",
        "# 3. CNN --> Pooling/Output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeMzaUd3pPRT",
        "colab_type": "text"
      },
      "source": [
        "### Train and dev"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJHVWeB_pKu7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9d30edc-4894-4dfe-a54c-fdd389c4b652"
      },
      "source": [
        "#! /usr/bin/env python\n",
        "# Parameters\n",
        "# ==================================================\n",
        "\n",
        "# Data loading params\n",
        "dev_size = .10\n",
        "\n",
        "# Model Hyperparameters\n",
        "embedding_dim  = 32     #128\n",
        "max_seq_legth = 70 \n",
        "filter_sizes = [3,4,5]  #3\n",
        "num_filters = 32\n",
        "dropout_prob = 0.5 #0.5\n",
        "l2_reg_lambda = 0.0\n",
        "use_glove = True #Do we use glove\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 128\n",
        "num_epochs = 10 #200\n",
        "evaluate_every = 100 #100\n",
        "checkpoint_every = 100000 #100\n",
        "num_checkpoints = 0 #Checkpoints to store\n",
        "\n",
        "\n",
        "# Misc Parameters\n",
        "allow_soft_placement = True\n",
        "log_device_placement = False\n",
        "\n",
        "\n",
        "\n",
        "# Data Preparation\n",
        "# ==================================================\n",
        "\n",
        "\n",
        "filename = \"./Sentiment Analysis Dataset.csv\"\n",
        "goodfile = \"./good_Sentiment Analysis Dataset.csv\"\n",
        "badfile = \"./bad_Sentiment Analysis Dataset.csv\"\n",
        "\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "x_text, y = get_dataset(goodfile, badfile, 5000) #TODO: MAX LENGTH\n",
        "\n",
        "# Build vocabulary\n",
        "# https://www.cnblogs.com/helloworld0604/p/9002337.html\n",
        "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
        "if (not use_glove):\n",
        "    print (\"Not using GloVe\")\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
        "else:\n",
        "    print (\"Using GloVe\")\n",
        "    embedding_dim = 50\n",
        "    filename = './glove.6B.50d.txt'\n",
        "    def loadGloVe(filename):\n",
        "        vocab = []\n",
        "        embd = []\n",
        "        file = open(filename,'r')\n",
        "        for line in file.readlines():\n",
        "            row = line.strip().split(' ')\n",
        "            vocab.append(row[0])\n",
        "            embd.append(row[1:])\n",
        "        print('Loaded GloVe!')\n",
        "        file.close()\n",
        "        return vocab,embd\n",
        "    vocab,embd = loadGloVe(filename)\n",
        "    vocab_size = len(vocab)\n",
        "    # embedding_dim = len(embd[0])\n",
        "    embedding = np.asarray(embd)\n",
        "\n",
        "    W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n",
        "                    trainable=False, name=\"W\")\n",
        "    embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
        "    embedding_init = W.assign(embedding_placeholder)\n",
        "\n",
        "    # https://blog.csdn.net/dcrmg/article/details/79091941\n",
        "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
        "    sess = tf.Session(config=session_conf)\n",
        "    sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n",
        "\n",
        "    from tensorflow.contrib import learn\n",
        "    #init vocab processor\n",
        "    #https://blog.csdn.net/The_lastest/article/details/81771723\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    #fit the vocab from glove\n",
        "    pretrain = vocab_processor.fit(vocab)\n",
        "    #transform inputs\n",
        "    x = np.array(list(vocab_processor.transform(x_text)))\n",
        "\n",
        "    #init vocab processor\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    #fit the vocab from glove\n",
        "    pretrain = vocab_processor.fit(vocab)\n",
        "    #transform inputs\n",
        "    x = np.array(list(vocab_processor.transform(x_text)))\n",
        "\n",
        "\n",
        "# Randomly shuffle data\n",
        "np.random.seed(42)\n",
        "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
        "x_shuffled = x[shuffle_indices]\n",
        "y_shuffled = y[shuffle_indices]\n",
        "\n",
        "# Split train/test set\n",
        "# TODO: This is very crude, should use cross-validation\n",
        "dev_sample_index = -1 * int(dev_size * float(len(y)))\n",
        "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
        "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
        "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
        "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
        "\n",
        "#embed()\n",
        "\n",
        "\n",
        "# Training\n",
        "# ==================================================\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
        "    sess = tf.Session(config=session_conf)\n",
        "    with sess.as_default():\n",
        "        \n",
        "        model = LSTM_CNN(x_train.shape[1],y_train.shape[1],len(vocab_processor.vocabulary_),embedding_dim,filter_sizes,num_filters,l2_reg_lambda)\n",
        "\n",
        "\n",
        "        # Define Training procedure\n",
        "        # https://blog.csdn.net/brucewong0516/article/details/78838124\n",
        "        # https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/train/AdamOptimizer\n",
        "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "        grads_and_vars = optimizer.compute_gradients(model.loss)\n",
        "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "        # Keep track of gradient values and sparsity (optional)\n",
        "        grad_summaries = []\n",
        "        for g, v in grads_and_vars:\n",
        "            if g is not None:\n",
        "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
        "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
        "                grad_summaries.append(grad_hist_summary)\n",
        "                grad_summaries.append(sparsity_summary)\n",
        "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
        "\n",
        "        # Output directory for models and summaries\n",
        "        timestamp = str(int(time.time()))\n",
        "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "        print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "        # Summaries for loss and accuracy\n",
        "        loss_summary = tf.summary.scalar(\"loss\", model.loss)\n",
        "        acc_summary = tf.summary.scalar(\"accuracy\", model.accuracy)\n",
        "\n",
        "        # Train Summaries\n",
        "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
        "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "        # Dev summaries\n",
        "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
        "\n",
        "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
        "\n",
        "        # Write vocabulary\n",
        "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
        "\n",
        "        # Initialize all variables\n",
        "        # https://blog.csdn.net/u012436149/article/details/78291545\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        #TRAINING STEP\n",
        "        def train_step(x_batch, y_batch,save=False):\n",
        "            feed_dict = {\n",
        "              model.input_x: x_batch,\n",
        "              model.input_y: y_batch,\n",
        "              model.dropout_keep_prob: dropout_prob\n",
        "            }\n",
        "            _, step, summaries, loss, accuracy = sess.run(\n",
        "                [train_op, global_step, train_summary_op, model.loss, model.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            if save:\n",
        "                train_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "        #EVALUATE MODEL\n",
        "        def dev_step(x_batch, y_batch, writer=None,save=False):\n",
        "            feed_dict = {\n",
        "              model.input_x: x_batch,\n",
        "              model.input_y: y_batch,\n",
        "              model.dropout_keep_prob: 0.5\n",
        "            }\n",
        "            step, summaries, loss, accuracy = sess.run(\n",
        "                [global_step, dev_summary_op, model.loss, model.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            if save:\n",
        "                if writer:\n",
        "                    writer.add_summary(summaries, step)\n",
        "\n",
        "        #CREATE THE BATCHES GENERATOR\n",
        "        batches = gen_batch(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
        "        \n",
        "        #TRAIN FOR EACH BATCH\n",
        "        for batch in batches:\n",
        "            x_batch, y_batch = zip(*batch)\n",
        "            train_step(x_batch, y_batch)\n",
        "            current_step = tf.train.global_step(sess, global_step)\n",
        "            if current_step % evaluate_every == 0:\n",
        "                print(\"\\nEvaluation:\")\n",
        "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
        "                print(\"\")\n",
        "            if current_step % checkpoint_every == 0:\n",
        "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
        "        dev_step(x_dev, y_dev, writer=dev_summary_writer)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Using GloVe\n",
            "Loaded GloVe!\n",
            "Vocabulary Size: 370847\n",
            "Train/Dev split: 9000/1000\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f269af64eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f269af64eb8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f269af64eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f269af64eb8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From <ipython-input-7-3aa2e7651e08>:70: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-7-3aa2e7651e08>:94: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "(!!) LOADED LSTM-CNN! :)\n",
            "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name rnn/lstm_cell/kernel:0/grad/hist is illegal; using rnn/lstm_cell/kernel_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name rnn/lstm_cell/kernel:0/grad/sparsity is illegal; using rnn/lstm_cell/kernel_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name rnn/lstm_cell/bias:0/grad/hist is illegal; using rnn/lstm_cell/bias_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name rnn/lstm_cell/bias:0/grad/sparsity is illegal; using rnn/lstm_cell/bias_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
            "Writing to /content/drive/My Drive/Colab Notebooks/TF1.4/data/Sentiment Analysis Dataset/runs/1570033624\n",
            "\n",
            "2019-10-02T16:27:07.171338: step 1, loss 0.716713, acc 0.484375\n",
            "2019-10-02T16:27:07.741006: step 2, loss 0.819627, acc 0.46875\n",
            "2019-10-02T16:27:08.309277: step 3, loss 0.747685, acc 0.5\n",
            "2019-10-02T16:27:08.845673: step 4, loss 0.709544, acc 0.5625\n",
            "2019-10-02T16:27:09.409556: step 5, loss 0.705083, acc 0.515625\n",
            "2019-10-02T16:27:09.946012: step 6, loss 0.781384, acc 0.460938\n",
            "2019-10-02T16:27:10.490547: step 7, loss 0.738377, acc 0.539062\n",
            "2019-10-02T16:27:11.030797: step 8, loss 0.753393, acc 0.460938\n",
            "2019-10-02T16:27:11.584514: step 9, loss 0.781865, acc 0.46875\n",
            "2019-10-02T16:27:12.153365: step 10, loss 0.727558, acc 0.53125\n",
            "2019-10-02T16:27:12.691549: step 11, loss 0.723652, acc 0.546875\n",
            "2019-10-02T16:27:13.235401: step 12, loss 0.742455, acc 0.429688\n",
            "2019-10-02T16:27:13.762454: step 13, loss 0.717728, acc 0.539062\n",
            "2019-10-02T16:27:14.328031: step 14, loss 0.731811, acc 0.523438\n",
            "2019-10-02T16:27:14.896256: step 15, loss 0.694928, acc 0.554688\n",
            "2019-10-02T16:27:15.480181: step 16, loss 0.701349, acc 0.546875\n",
            "2019-10-02T16:27:16.055994: step 17, loss 0.72914, acc 0.484375\n",
            "2019-10-02T16:27:16.597566: step 18, loss 0.701746, acc 0.578125\n",
            "2019-10-02T16:27:17.145773: step 19, loss 0.690416, acc 0.53125\n",
            "2019-10-02T16:27:17.700945: step 20, loss 0.68366, acc 0.539062\n",
            "2019-10-02T16:27:18.285199: step 21, loss 0.691936, acc 0.554688\n",
            "2019-10-02T16:27:18.861575: step 22, loss 0.696052, acc 0.492188\n",
            "2019-10-02T16:27:19.400659: step 23, loss 0.691601, acc 0.53125\n",
            "2019-10-02T16:27:19.945453: step 24, loss 0.697233, acc 0.53125\n",
            "2019-10-02T16:27:20.483116: step 25, loss 0.740156, acc 0.484375\n",
            "2019-10-02T16:27:21.023016: step 26, loss 0.703445, acc 0.570312\n",
            "2019-10-02T16:27:21.553760: step 27, loss 0.734137, acc 0.539062\n",
            "2019-10-02T16:27:22.107170: step 28, loss 0.724541, acc 0.515625\n",
            "2019-10-02T16:27:22.654871: step 29, loss 0.693959, acc 0.554688\n",
            "2019-10-02T16:27:23.206129: step 30, loss 0.694565, acc 0.5625\n",
            "2019-10-02T16:27:23.755492: step 31, loss 0.720601, acc 0.453125\n",
            "2019-10-02T16:27:24.293673: step 32, loss 0.710118, acc 0.539062\n",
            "2019-10-02T16:27:24.850166: step 33, loss 0.696077, acc 0.554688\n",
            "2019-10-02T16:27:25.388024: step 34, loss 0.712041, acc 0.515625\n",
            "2019-10-02T16:27:25.924990: step 35, loss 0.672429, acc 0.578125\n",
            "2019-10-02T16:27:26.463373: step 36, loss 0.773428, acc 0.398438\n",
            "2019-10-02T16:27:27.001417: step 37, loss 0.740323, acc 0.5\n",
            "2019-10-02T16:27:27.530929: step 38, loss 0.680216, acc 0.609375\n",
            "2019-10-02T16:27:28.078972: step 39, loss 0.678135, acc 0.578125\n",
            "2019-10-02T16:27:28.651636: step 40, loss 0.701022, acc 0.546875\n",
            "2019-10-02T16:27:29.187211: step 41, loss 0.685181, acc 0.546875\n",
            "2019-10-02T16:27:29.719054: step 42, loss 0.686034, acc 0.570312\n",
            "2019-10-02T16:27:30.263309: step 43, loss 0.711041, acc 0.5\n",
            "2019-10-02T16:27:30.818185: step 44, loss 0.701696, acc 0.5\n",
            "2019-10-02T16:27:31.344733: step 45, loss 0.689611, acc 0.554688\n",
            "2019-10-02T16:27:31.905007: step 46, loss 0.692241, acc 0.546875\n",
            "2019-10-02T16:27:32.434297: step 47, loss 0.679597, acc 0.664062\n",
            "2019-10-02T16:27:32.991102: step 48, loss 0.692307, acc 0.601562\n",
            "2019-10-02T16:27:33.548052: step 49, loss 0.707499, acc 0.578125\n",
            "2019-10-02T16:27:34.103121: step 50, loss 0.667888, acc 0.609375\n",
            "2019-10-02T16:27:34.663912: step 51, loss 0.66693, acc 0.632812\n",
            "2019-10-02T16:27:35.218875: step 52, loss 0.687985, acc 0.578125\n",
            "2019-10-02T16:27:35.774217: step 53, loss 0.682172, acc 0.53125\n",
            "2019-10-02T16:27:36.330840: step 54, loss 0.662023, acc 0.617188\n",
            "2019-10-02T16:27:36.864327: step 55, loss 0.698046, acc 0.554688\n",
            "2019-10-02T16:27:37.415500: step 56, loss 0.676483, acc 0.539062\n",
            "2019-10-02T16:27:37.964039: step 57, loss 0.657766, acc 0.617188\n",
            "2019-10-02T16:27:38.490918: step 58, loss 0.677263, acc 0.585938\n",
            "2019-10-02T16:27:39.058203: step 59, loss 0.691009, acc 0.570312\n",
            "2019-10-02T16:27:39.595152: step 60, loss 0.676181, acc 0.570312\n",
            "2019-10-02T16:27:40.140561: step 61, loss 0.644254, acc 0.671875\n",
            "2019-10-02T16:27:40.648130: step 62, loss 0.686662, acc 0.554688\n",
            "2019-10-02T16:27:41.169537: step 63, loss 0.684803, acc 0.585938\n",
            "2019-10-02T16:27:41.684745: step 64, loss 0.660588, acc 0.609375\n",
            "2019-10-02T16:27:42.243642: step 65, loss 0.676194, acc 0.609375\n",
            "2019-10-02T16:27:42.784345: step 66, loss 0.669057, acc 0.609375\n",
            "2019-10-02T16:27:43.331316: step 67, loss 0.636613, acc 0.65625\n",
            "2019-10-02T16:27:43.878603: step 68, loss 0.666119, acc 0.601562\n",
            "2019-10-02T16:27:44.428407: step 69, loss 0.643704, acc 0.625\n",
            "2019-10-02T16:27:44.973959: step 70, loss 0.649765, acc 0.625\n",
            "2019-10-02T16:27:45.571320: step 71, loss 0.68232, acc 0.525\n",
            "2019-10-02T16:27:46.124565: step 72, loss 0.685552, acc 0.554688\n",
            "2019-10-02T16:27:46.650907: step 73, loss 0.646594, acc 0.601562\n",
            "2019-10-02T16:27:47.209217: step 74, loss 0.638814, acc 0.671875\n",
            "2019-10-02T16:27:47.744349: step 75, loss 0.690659, acc 0.601562\n",
            "2019-10-02T16:27:48.294200: step 76, loss 0.656873, acc 0.609375\n",
            "2019-10-02T16:27:48.849073: step 77, loss 0.6336, acc 0.617188\n",
            "2019-10-02T16:27:49.429502: step 78, loss 0.656057, acc 0.546875\n",
            "2019-10-02T16:27:49.993815: step 79, loss 0.648532, acc 0.59375\n",
            "2019-10-02T16:27:50.542293: step 80, loss 0.623762, acc 0.65625\n",
            "2019-10-02T16:27:51.075401: step 81, loss 0.658299, acc 0.601562\n",
            "2019-10-02T16:27:51.616246: step 82, loss 0.673706, acc 0.570312\n",
            "2019-10-02T16:27:52.144754: step 83, loss 0.686609, acc 0.554688\n",
            "2019-10-02T16:27:52.699061: step 84, loss 0.672316, acc 0.539062\n",
            "2019-10-02T16:27:53.251610: step 85, loss 0.666021, acc 0.609375\n",
            "2019-10-02T16:27:53.790408: step 86, loss 0.656909, acc 0.640625\n",
            "2019-10-02T16:27:54.325081: step 87, loss 0.686492, acc 0.570312\n",
            "2019-10-02T16:27:54.870397: step 88, loss 0.635279, acc 0.570312\n",
            "2019-10-02T16:27:55.368441: step 89, loss 0.652185, acc 0.5625\n",
            "2019-10-02T16:27:55.920017: step 90, loss 0.633555, acc 0.648438\n",
            "2019-10-02T16:27:56.458094: step 91, loss 0.648112, acc 0.609375\n",
            "2019-10-02T16:27:57.023100: step 92, loss 0.68215, acc 0.617188\n",
            "2019-10-02T16:27:57.596821: step 93, loss 0.618605, acc 0.648438\n",
            "2019-10-02T16:27:58.121211: step 94, loss 0.642908, acc 0.609375\n",
            "2019-10-02T16:27:58.666675: step 95, loss 0.57628, acc 0.726562\n",
            "2019-10-02T16:27:59.227415: step 96, loss 0.650504, acc 0.617188\n",
            "2019-10-02T16:27:59.781506: step 97, loss 0.675028, acc 0.585938\n",
            "2019-10-02T16:28:00.346898: step 98, loss 0.662937, acc 0.648438\n",
            "2019-10-02T16:28:00.891003: step 99, loss 0.591555, acc 0.6875\n",
            "2019-10-02T16:28:01.408945: step 100, loss 0.611049, acc 0.703125\n",
            "\n",
            "Evaluation:\n",
            "2019-10-02T16:28:01.614415: step 100, loss 0.654592, acc 0.6\n",
            "\n",
            "2019-10-02T16:28:02.158339: step 101, loss 0.628556, acc 0.617188\n",
            "2019-10-02T16:28:02.700425: step 102, loss 0.613524, acc 0.671875\n",
            "2019-10-02T16:28:03.213026: step 103, loss 0.66294, acc 0.578125\n",
            "2019-10-02T16:28:03.769424: step 104, loss 0.629018, acc 0.664062\n",
            "2019-10-02T16:28:04.323136: step 105, loss 0.650503, acc 0.640625\n",
            "2019-10-02T16:28:04.845946: step 106, loss 0.651955, acc 0.632812\n",
            "2019-10-02T16:28:05.363253: step 107, loss 0.634685, acc 0.59375\n",
            "2019-10-02T16:28:05.904537: step 108, loss 0.651221, acc 0.617188\n",
            "2019-10-02T16:28:06.447194: step 109, loss 0.638624, acc 0.664062\n",
            "2019-10-02T16:28:06.996417: step 110, loss 0.636407, acc 0.632812\n",
            "2019-10-02T16:28:07.539740: step 111, loss 0.663767, acc 0.601562\n",
            "2019-10-02T16:28:08.087394: step 112, loss 0.638041, acc 0.625\n",
            "2019-10-02T16:28:08.634355: step 113, loss 0.663016, acc 0.640625\n",
            "2019-10-02T16:28:09.164676: step 114, loss 0.646996, acc 0.617188\n",
            "2019-10-02T16:28:09.704442: step 115, loss 0.674227, acc 0.59375\n",
            "2019-10-02T16:28:10.237099: step 116, loss 0.633097, acc 0.632812\n",
            "2019-10-02T16:28:10.809338: step 117, loss 0.631353, acc 0.609375\n",
            "2019-10-02T16:28:11.336544: step 118, loss 0.628945, acc 0.648438\n",
            "2019-10-02T16:28:11.877233: step 119, loss 0.626516, acc 0.632812\n",
            "2019-10-02T16:28:12.420118: step 120, loss 0.661057, acc 0.609375\n",
            "2019-10-02T16:28:12.952405: step 121, loss 0.651044, acc 0.609375\n",
            "2019-10-02T16:28:13.490152: step 122, loss 0.613053, acc 0.664062\n",
            "2019-10-02T16:28:14.027111: step 123, loss 0.637356, acc 0.664062\n",
            "2019-10-02T16:28:14.561934: step 124, loss 0.672453, acc 0.570312\n",
            "2019-10-02T16:28:15.110164: step 125, loss 0.649021, acc 0.601562\n",
            "2019-10-02T16:28:15.637975: step 126, loss 0.677191, acc 0.601562\n",
            "2019-10-02T16:28:16.193794: step 127, loss 0.719155, acc 0.53125\n",
            "2019-10-02T16:28:16.716859: step 128, loss 0.644101, acc 0.601562\n",
            "2019-10-02T16:28:17.273156: step 129, loss 0.657922, acc 0.632812\n",
            "2019-10-02T16:28:17.826800: step 130, loss 0.616139, acc 0.703125\n",
            "2019-10-02T16:28:18.374193: step 131, loss 0.670198, acc 0.5625\n",
            "2019-10-02T16:28:18.903078: step 132, loss 0.605482, acc 0.671875\n",
            "2019-10-02T16:28:19.462120: step 133, loss 0.614054, acc 0.671875\n",
            "2019-10-02T16:28:19.993173: step 134, loss 0.616958, acc 0.664062\n",
            "2019-10-02T16:28:20.546872: step 135, loss 0.690133, acc 0.601562\n",
            "2019-10-02T16:28:21.110858: step 136, loss 0.651164, acc 0.609375\n",
            "2019-10-02T16:28:21.636423: step 137, loss 0.594473, acc 0.695312\n",
            "2019-10-02T16:28:22.177193: step 138, loss 0.664006, acc 0.570312\n",
            "2019-10-02T16:28:22.710436: step 139, loss 0.634588, acc 0.625\n",
            "2019-10-02T16:28:23.255097: step 140, loss 0.626459, acc 0.671875\n",
            "2019-10-02T16:28:23.814964: step 141, loss 0.578131, acc 0.695312\n",
            "2019-10-02T16:28:24.368153: step 142, loss 0.706414, acc 0.575\n",
            "2019-10-02T16:28:24.908097: step 143, loss 0.63515, acc 0.609375\n",
            "2019-10-02T16:28:25.461727: step 144, loss 0.632786, acc 0.65625\n",
            "2019-10-02T16:28:25.976664: step 145, loss 0.574063, acc 0.710938\n",
            "2019-10-02T16:28:26.523799: step 146, loss 0.623657, acc 0.632812\n",
            "2019-10-02T16:28:27.082421: step 147, loss 0.661924, acc 0.617188\n",
            "2019-10-02T16:28:27.627129: step 148, loss 0.645007, acc 0.59375\n",
            "2019-10-02T16:28:28.158396: step 149, loss 0.604158, acc 0.6875\n",
            "2019-10-02T16:28:28.694611: step 150, loss 0.593983, acc 0.726562\n",
            "2019-10-02T16:28:29.219010: step 151, loss 0.558743, acc 0.710938\n",
            "2019-10-02T16:28:29.757330: step 152, loss 0.616124, acc 0.679688\n",
            "2019-10-02T16:28:30.291952: step 153, loss 0.602227, acc 0.664062\n",
            "2019-10-02T16:28:30.836584: step 154, loss 0.583602, acc 0.671875\n",
            "2019-10-02T16:28:31.379134: step 155, loss 0.644831, acc 0.648438\n",
            "2019-10-02T16:28:31.929400: step 156, loss 0.600047, acc 0.671875\n",
            "2019-10-02T16:28:32.449406: step 157, loss 0.610437, acc 0.65625\n",
            "2019-10-02T16:28:33.005087: step 158, loss 0.612787, acc 0.648438\n",
            "2019-10-02T16:28:33.549620: step 159, loss 0.623543, acc 0.617188\n",
            "2019-10-02T16:28:34.093616: step 160, loss 0.611015, acc 0.632812\n",
            "2019-10-02T16:28:34.650535: step 161, loss 0.630739, acc 0.664062\n",
            "2019-10-02T16:28:35.224151: step 162, loss 0.609232, acc 0.703125\n",
            "2019-10-02T16:28:35.783558: step 163, loss 0.632061, acc 0.617188\n",
            "2019-10-02T16:28:36.313861: step 164, loss 0.548419, acc 0.726562\n",
            "2019-10-02T16:28:36.860481: step 165, loss 0.642165, acc 0.648438\n",
            "2019-10-02T16:28:37.418594: step 166, loss 0.615371, acc 0.640625\n",
            "2019-10-02T16:28:37.961563: step 167, loss 0.563108, acc 0.703125\n",
            "2019-10-02T16:28:38.487140: step 168, loss 0.626294, acc 0.65625\n",
            "2019-10-02T16:28:39.017192: step 169, loss 0.632841, acc 0.648438\n",
            "2019-10-02T16:28:39.563673: step 170, loss 0.577689, acc 0.671875\n",
            "2019-10-02T16:28:40.108444: step 171, loss 0.596032, acc 0.671875\n",
            "2019-10-02T16:28:40.638422: step 172, loss 0.565881, acc 0.6875\n",
            "2019-10-02T16:28:41.180612: step 173, loss 0.658125, acc 0.59375\n",
            "2019-10-02T16:28:41.717777: step 174, loss 0.620279, acc 0.664062\n",
            "2019-10-02T16:28:42.290238: step 175, loss 0.601644, acc 0.679688\n",
            "2019-10-02T16:28:42.834034: step 176, loss 0.635569, acc 0.664062\n",
            "2019-10-02T16:28:43.386352: step 177, loss 0.557795, acc 0.664062\n",
            "2019-10-02T16:28:43.929502: step 178, loss 0.638462, acc 0.601562\n",
            "2019-10-02T16:28:44.488941: step 179, loss 0.559773, acc 0.734375\n",
            "2019-10-02T16:28:45.054634: step 180, loss 0.63858, acc 0.617188\n",
            "2019-10-02T16:28:45.585537: step 181, loss 0.680194, acc 0.664062\n",
            "2019-10-02T16:28:46.134282: step 182, loss 0.638479, acc 0.609375\n",
            "2019-10-02T16:28:46.660385: step 183, loss 0.589264, acc 0.6875\n",
            "2019-10-02T16:28:47.206345: step 184, loss 0.673573, acc 0.625\n",
            "2019-10-02T16:28:47.747903: step 185, loss 0.577381, acc 0.679688\n",
            "2019-10-02T16:28:48.304997: step 186, loss 0.57452, acc 0.695312\n",
            "2019-10-02T16:28:48.840664: step 187, loss 0.59157, acc 0.664062\n",
            "2019-10-02T16:28:49.387684: step 188, loss 0.59072, acc 0.671875\n",
            "2019-10-02T16:28:49.935071: step 189, loss 0.600987, acc 0.671875\n",
            "2019-10-02T16:28:50.481884: step 190, loss 0.571244, acc 0.71875\n",
            "2019-10-02T16:28:51.020499: step 191, loss 0.599211, acc 0.695312\n",
            "2019-10-02T16:28:51.564502: step 192, loss 0.578461, acc 0.726562\n",
            "2019-10-02T16:28:52.097250: step 193, loss 0.626432, acc 0.632812\n",
            "2019-10-02T16:28:52.673666: step 194, loss 0.633866, acc 0.609375\n",
            "2019-10-02T16:28:53.207039: step 195, loss 0.644762, acc 0.640625\n",
            "2019-10-02T16:28:53.758724: step 196, loss 0.556878, acc 0.742188\n",
            "2019-10-02T16:28:54.282977: step 197, loss 0.553095, acc 0.757812\n",
            "2019-10-02T16:28:54.836582: step 198, loss 0.557284, acc 0.726562\n",
            "2019-10-02T16:28:55.402806: step 199, loss 0.568627, acc 0.710938\n",
            "2019-10-02T16:28:55.938871: step 200, loss 0.554761, acc 0.71875\n",
            "\n",
            "Evaluation:\n",
            "2019-10-02T16:28:55.982534: step 200, loss 0.616819, acc 0.653\n",
            "\n",
            "2019-10-02T16:28:56.519214: step 201, loss 0.635056, acc 0.648438\n",
            "2019-10-02T16:28:57.038801: step 202, loss 0.567578, acc 0.703125\n",
            "2019-10-02T16:28:57.597283: step 203, loss 0.642976, acc 0.648438\n",
            "2019-10-02T16:28:58.132770: step 204, loss 0.600836, acc 0.632812\n",
            "2019-10-02T16:28:58.692719: step 205, loss 0.593655, acc 0.632812\n",
            "2019-10-02T16:28:59.207400: step 206, loss 0.596589, acc 0.664062\n",
            "2019-10-02T16:28:59.768446: step 207, loss 0.529071, acc 0.734375\n",
            "2019-10-02T16:29:00.307369: step 208, loss 0.630733, acc 0.632812\n",
            "2019-10-02T16:29:00.858502: step 209, loss 0.630396, acc 0.640625\n",
            "2019-10-02T16:29:01.405962: step 210, loss 0.61942, acc 0.632812\n",
            "2019-10-02T16:29:01.939530: step 211, loss 0.645938, acc 0.632812\n",
            "2019-10-02T16:29:02.476792: step 212, loss 0.549188, acc 0.71875\n",
            "2019-10-02T16:29:03.038482: step 213, loss 0.641266, acc 0.7\n",
            "2019-10-02T16:29:03.618720: step 214, loss 0.545986, acc 0.742188\n",
            "2019-10-02T16:29:04.162832: step 215, loss 0.548399, acc 0.742188\n",
            "2019-10-02T16:29:04.706549: step 216, loss 0.586423, acc 0.679688\n",
            "2019-10-02T16:29:05.266173: step 217, loss 0.560331, acc 0.703125\n",
            "2019-10-02T16:29:05.827309: step 218, loss 0.560084, acc 0.65625\n",
            "2019-10-02T16:29:06.364329: step 219, loss 0.610571, acc 0.609375\n",
            "2019-10-02T16:29:06.902169: step 220, loss 0.606726, acc 0.71875\n",
            "2019-10-02T16:29:07.443559: step 221, loss 0.581158, acc 0.703125\n",
            "2019-10-02T16:29:07.992770: step 222, loss 0.564735, acc 0.71875\n",
            "2019-10-02T16:29:08.544839: step 223, loss 0.539676, acc 0.726562\n",
            "2019-10-02T16:29:09.105514: step 224, loss 0.564849, acc 0.703125\n",
            "2019-10-02T16:29:09.637953: step 225, loss 0.524961, acc 0.71875\n",
            "2019-10-02T16:29:10.193091: step 226, loss 0.602809, acc 0.648438\n",
            "2019-10-02T16:29:10.720009: step 227, loss 0.559781, acc 0.6875\n",
            "2019-10-02T16:29:11.278170: step 228, loss 0.595618, acc 0.65625\n",
            "2019-10-02T16:29:11.829066: step 229, loss 0.496822, acc 0.75\n",
            "2019-10-02T16:29:12.389441: step 230, loss 0.580658, acc 0.65625\n",
            "2019-10-02T16:29:12.936925: step 231, loss 0.543868, acc 0.71875\n",
            "2019-10-02T16:29:13.484219: step 232, loss 0.578646, acc 0.679688\n",
            "2019-10-02T16:29:14.056829: step 233, loss 0.568324, acc 0.6875\n",
            "2019-10-02T16:29:14.594101: step 234, loss 0.547033, acc 0.710938\n",
            "2019-10-02T16:29:15.135431: step 235, loss 0.600753, acc 0.710938\n",
            "2019-10-02T16:29:15.673379: step 236, loss 0.473508, acc 0.820312\n",
            "2019-10-02T16:29:16.207521: step 237, loss 0.518986, acc 0.71875\n",
            "2019-10-02T16:29:16.758578: step 238, loss 0.536799, acc 0.734375\n",
            "2019-10-02T16:29:17.293813: step 239, loss 0.574769, acc 0.65625\n",
            "2019-10-02T16:29:17.849470: step 240, loss 0.584907, acc 0.65625\n",
            "2019-10-02T16:29:18.419093: step 241, loss 0.603897, acc 0.648438\n",
            "2019-10-02T16:29:18.972552: step 242, loss 0.586817, acc 0.679688\n",
            "2019-10-02T16:29:19.531063: step 243, loss 0.555769, acc 0.726562\n",
            "2019-10-02T16:29:20.114443: step 244, loss 0.586113, acc 0.640625\n",
            "2019-10-02T16:29:20.654784: step 245, loss 0.486574, acc 0.742188\n",
            "2019-10-02T16:29:21.209236: step 246, loss 0.507568, acc 0.757812\n",
            "2019-10-02T16:29:21.738775: step 247, loss 0.506394, acc 0.757812\n",
            "2019-10-02T16:29:22.305974: step 248, loss 0.574352, acc 0.734375\n",
            "2019-10-02T16:29:22.861839: step 249, loss 0.53257, acc 0.742188\n",
            "2019-10-02T16:29:23.432571: step 250, loss 0.583797, acc 0.710938\n",
            "2019-10-02T16:29:23.974301: step 251, loss 0.551815, acc 0.71875\n",
            "2019-10-02T16:29:24.551959: step 252, loss 0.531735, acc 0.710938\n",
            "2019-10-02T16:29:25.119284: step 253, loss 0.593835, acc 0.6875\n",
            "2019-10-02T16:29:25.657930: step 254, loss 0.581615, acc 0.640625\n",
            "2019-10-02T16:29:26.191602: step 255, loss 0.540616, acc 0.726562\n",
            "2019-10-02T16:29:26.736524: step 256, loss 0.601646, acc 0.695312\n",
            "2019-10-02T16:29:27.274483: step 257, loss 0.502087, acc 0.765625\n",
            "2019-10-02T16:29:27.825922: step 258, loss 0.560138, acc 0.695312\n",
            "2019-10-02T16:29:28.374973: step 259, loss 0.558976, acc 0.742188\n",
            "2019-10-02T16:29:28.898816: step 260, loss 0.562478, acc 0.679688\n",
            "2019-10-02T16:29:29.457567: step 261, loss 0.526105, acc 0.75\n",
            "2019-10-02T16:29:30.010441: step 262, loss 0.571101, acc 0.703125\n",
            "2019-10-02T16:29:30.535244: step 263, loss 0.480009, acc 0.789062\n",
            "2019-10-02T16:29:31.072471: step 264, loss 0.544658, acc 0.6875\n",
            "2019-10-02T16:29:31.620522: step 265, loss 0.519372, acc 0.710938\n",
            "2019-10-02T16:29:32.163610: step 266, loss 0.569635, acc 0.726562\n",
            "2019-10-02T16:29:32.725106: step 267, loss 0.575158, acc 0.6875\n",
            "2019-10-02T16:29:33.289445: step 268, loss 0.531951, acc 0.757812\n",
            "2019-10-02T16:29:33.857313: step 269, loss 0.514886, acc 0.742188\n",
            "2019-10-02T16:29:34.423684: step 270, loss 0.617482, acc 0.6875\n",
            "2019-10-02T16:29:35.009871: step 271, loss 0.560651, acc 0.734375\n",
            "2019-10-02T16:29:35.590543: step 272, loss 0.544083, acc 0.71875\n",
            "2019-10-02T16:29:36.137412: step 273, loss 0.582144, acc 0.671875\n",
            "2019-10-02T16:29:36.689908: step 274, loss 0.673106, acc 0.6875\n",
            "2019-10-02T16:29:37.248839: step 275, loss 0.542949, acc 0.695312\n",
            "2019-10-02T16:29:37.803819: step 276, loss 0.518162, acc 0.726562\n",
            "2019-10-02T16:29:38.343934: step 277, loss 0.515348, acc 0.773438\n",
            "2019-10-02T16:29:38.911258: step 278, loss 0.537948, acc 0.734375\n",
            "2019-10-02T16:29:39.475450: step 279, loss 0.497761, acc 0.757812\n",
            "2019-10-02T16:29:40.040068: step 280, loss 0.552714, acc 0.726562\n",
            "2019-10-02T16:29:40.597358: step 281, loss 0.597377, acc 0.679688\n",
            "2019-10-02T16:29:41.148271: step 282, loss 0.51797, acc 0.757812\n",
            "2019-10-02T16:29:41.685409: step 283, loss 0.564331, acc 0.742188\n",
            "2019-10-02T16:29:42.235279: step 284, loss 0.503269, acc 0.775\n",
            "2019-10-02T16:29:42.791199: step 285, loss 0.566449, acc 0.710938\n",
            "2019-10-02T16:29:43.346122: step 286, loss 0.462624, acc 0.765625\n",
            "2019-10-02T16:29:43.913978: step 287, loss 0.572812, acc 0.6875\n",
            "2019-10-02T16:29:44.453054: step 288, loss 0.457175, acc 0.804688\n",
            "2019-10-02T16:29:45.007440: step 289, loss 0.496788, acc 0.742188\n",
            "2019-10-02T16:29:45.571956: step 290, loss 0.52636, acc 0.710938\n",
            "2019-10-02T16:29:46.143055: step 291, loss 0.526828, acc 0.703125\n",
            "2019-10-02T16:29:46.698964: step 292, loss 0.497315, acc 0.765625\n",
            "2019-10-02T16:29:47.268635: step 293, loss 0.575691, acc 0.695312\n",
            "2019-10-02T16:29:47.816728: step 294, loss 0.477839, acc 0.75\n",
            "2019-10-02T16:29:48.375115: step 295, loss 0.481679, acc 0.796875\n",
            "2019-10-02T16:29:48.924851: step 296, loss 0.490046, acc 0.789062\n",
            "2019-10-02T16:29:49.507809: step 297, loss 0.507876, acc 0.742188\n",
            "2019-10-02T16:29:50.052890: step 298, loss 0.492809, acc 0.789062\n",
            "2019-10-02T16:29:50.599266: step 299, loss 0.473049, acc 0.78125\n",
            "2019-10-02T16:29:51.159999: step 300, loss 0.487368, acc 0.796875\n",
            "\n",
            "Evaluation:\n",
            "2019-10-02T16:29:51.198516: step 300, loss 0.581383, acc 0.692\n",
            "\n",
            "2019-10-02T16:29:51.738406: step 301, loss 0.561323, acc 0.71875\n",
            "2019-10-02T16:29:52.312600: step 302, loss 0.532831, acc 0.726562\n",
            "2019-10-02T16:29:52.866025: step 303, loss 0.517329, acc 0.734375\n",
            "2019-10-02T16:29:53.425952: step 304, loss 0.457554, acc 0.804688\n",
            "2019-10-02T16:29:53.989319: step 305, loss 0.499147, acc 0.742188\n",
            "2019-10-02T16:29:54.564353: step 306, loss 0.473543, acc 0.765625\n",
            "2019-10-02T16:29:55.120679: step 307, loss 0.509228, acc 0.734375\n",
            "2019-10-02T16:29:55.672554: step 308, loss 0.548056, acc 0.71875\n",
            "2019-10-02T16:29:56.256013: step 309, loss 0.574823, acc 0.695312\n",
            "2019-10-02T16:29:56.809882: step 310, loss 0.565991, acc 0.664062\n",
            "2019-10-02T16:29:57.354843: step 311, loss 0.44659, acc 0.796875\n",
            "2019-10-02T16:29:57.898657: step 312, loss 0.523415, acc 0.757812\n",
            "2019-10-02T16:29:58.461119: step 313, loss 0.532237, acc 0.726562\n",
            "2019-10-02T16:29:59.014248: step 314, loss 0.562219, acc 0.742188\n",
            "2019-10-02T16:29:59.580507: step 315, loss 0.490156, acc 0.757812\n",
            "2019-10-02T16:30:00.152820: step 316, loss 0.53382, acc 0.734375\n",
            "2019-10-02T16:30:00.680034: step 317, loss 0.494392, acc 0.726562\n",
            "2019-10-02T16:30:01.209416: step 318, loss 0.451387, acc 0.8125\n",
            "2019-10-02T16:30:01.762595: step 319, loss 0.493092, acc 0.78125\n",
            "2019-10-02T16:30:02.316942: step 320, loss 0.485446, acc 0.773438\n",
            "2019-10-02T16:30:02.865752: step 321, loss 0.469083, acc 0.742188\n",
            "2019-10-02T16:30:03.421998: step 322, loss 0.432599, acc 0.757812\n",
            "2019-10-02T16:30:03.960556: step 323, loss 0.535185, acc 0.75\n",
            "2019-10-02T16:30:04.517557: step 324, loss 0.404147, acc 0.835938\n",
            "2019-10-02T16:30:05.041168: step 325, loss 0.506502, acc 0.765625\n",
            "2019-10-02T16:30:05.590170: step 326, loss 0.522703, acc 0.757812\n",
            "2019-10-02T16:30:06.126818: step 327, loss 0.483873, acc 0.765625\n",
            "2019-10-02T16:30:06.706565: step 328, loss 0.685465, acc 0.664062\n",
            "2019-10-02T16:30:07.261943: step 329, loss 0.451213, acc 0.78125\n",
            "2019-10-02T16:30:07.818241: step 330, loss 0.415816, acc 0.804688\n",
            "2019-10-02T16:30:08.355498: step 331, loss 0.50293, acc 0.796875\n",
            "2019-10-02T16:30:08.909925: step 332, loss 0.528849, acc 0.734375\n",
            "2019-10-02T16:30:09.470436: step 333, loss 0.506043, acc 0.75\n",
            "2019-10-02T16:30:10.030376: step 334, loss 0.532621, acc 0.71875\n",
            "2019-10-02T16:30:10.574291: step 335, loss 0.521952, acc 0.742188\n",
            "2019-10-02T16:30:11.133462: step 336, loss 0.476735, acc 0.8125\n",
            "2019-10-02T16:30:11.687844: step 337, loss 0.501678, acc 0.757812\n",
            "2019-10-02T16:30:12.250872: step 338, loss 0.482447, acc 0.789062\n",
            "2019-10-02T16:30:12.809161: step 339, loss 0.5357, acc 0.710938\n",
            "2019-10-02T16:30:13.381544: step 340, loss 0.479119, acc 0.765625\n",
            "2019-10-02T16:30:13.956774: step 341, loss 0.454605, acc 0.78125\n",
            "2019-10-02T16:30:14.504219: step 342, loss 0.528918, acc 0.6875\n",
            "2019-10-02T16:30:15.047933: step 343, loss 0.506938, acc 0.757812\n",
            "2019-10-02T16:30:15.604266: step 344, loss 0.48152, acc 0.742188\n",
            "2019-10-02T16:30:16.149934: step 345, loss 0.493167, acc 0.726562\n",
            "2019-10-02T16:30:16.694526: step 346, loss 0.519028, acc 0.757812\n",
            "2019-10-02T16:30:17.239957: step 347, loss 0.512271, acc 0.742188\n",
            "2019-10-02T16:30:17.772353: step 348, loss 0.457295, acc 0.75\n",
            "2019-10-02T16:30:18.307858: step 349, loss 0.459281, acc 0.765625\n",
            "2019-10-02T16:30:18.851729: step 350, loss 0.504941, acc 0.726562\n",
            "2019-10-02T16:30:19.419356: step 351, loss 0.450201, acc 0.789062\n",
            "2019-10-02T16:30:19.974250: step 352, loss 0.557702, acc 0.757812\n",
            "2019-10-02T16:30:20.524328: step 353, loss 0.54213, acc 0.695312\n",
            "2019-10-02T16:30:21.040819: step 354, loss 0.398643, acc 0.835938\n",
            "2019-10-02T16:30:21.564478: step 355, loss 0.464333, acc 0.775\n",
            "2019-10-02T16:30:22.117577: step 356, loss 0.416703, acc 0.820312\n",
            "2019-10-02T16:30:22.633048: step 357, loss 0.446722, acc 0.757812\n",
            "2019-10-02T16:30:23.182362: step 358, loss 0.445717, acc 0.804688\n",
            "2019-10-02T16:30:23.722094: step 359, loss 0.526246, acc 0.726562\n",
            "2019-10-02T16:30:24.271514: step 360, loss 0.400042, acc 0.828125\n",
            "2019-10-02T16:30:24.809298: step 361, loss 0.462304, acc 0.742188\n",
            "2019-10-02T16:30:25.340881: step 362, loss 0.476708, acc 0.757812\n",
            "2019-10-02T16:30:25.865177: step 363, loss 0.418264, acc 0.820312\n",
            "2019-10-02T16:30:26.432440: step 364, loss 0.486158, acc 0.75\n",
            "2019-10-02T16:30:26.973235: step 365, loss 0.392403, acc 0.804688\n",
            "2019-10-02T16:30:27.535284: step 366, loss 0.398038, acc 0.828125\n",
            "2019-10-02T16:30:28.098481: step 367, loss 0.493719, acc 0.773438\n",
            "2019-10-02T16:30:28.642808: step 368, loss 0.467912, acc 0.742188\n",
            "2019-10-02T16:30:29.190114: step 369, loss 0.39903, acc 0.820312\n",
            "2019-10-02T16:30:29.731280: step 370, loss 0.541276, acc 0.726562\n",
            "2019-10-02T16:30:30.297326: step 371, loss 0.353507, acc 0.820312\n",
            "2019-10-02T16:30:30.846925: step 372, loss 0.424949, acc 0.804688\n",
            "2019-10-02T16:30:31.380838: step 373, loss 0.474267, acc 0.75\n",
            "2019-10-02T16:30:31.933288: step 374, loss 0.467675, acc 0.796875\n",
            "2019-10-02T16:30:32.476194: step 375, loss 0.451251, acc 0.796875\n",
            "2019-10-02T16:30:33.031594: step 376, loss 0.454828, acc 0.78125\n",
            "2019-10-02T16:30:33.608067: step 377, loss 0.466527, acc 0.757812\n",
            "2019-10-02T16:30:34.162833: step 378, loss 0.463024, acc 0.75\n",
            "2019-10-02T16:30:34.729247: step 379, loss 0.488703, acc 0.71875\n",
            "2019-10-02T16:30:35.274819: step 380, loss 0.418487, acc 0.835938\n",
            "2019-10-02T16:30:35.852976: step 381, loss 0.392872, acc 0.851562\n",
            "2019-10-02T16:30:36.389041: step 382, loss 0.462292, acc 0.78125\n",
            "2019-10-02T16:30:36.959006: step 383, loss 0.444516, acc 0.796875\n",
            "2019-10-02T16:30:37.514496: step 384, loss 0.394765, acc 0.820312\n",
            "2019-10-02T16:30:38.057465: step 385, loss 0.460672, acc 0.804688\n",
            "2019-10-02T16:30:38.631376: step 386, loss 0.422592, acc 0.796875\n",
            "2019-10-02T16:30:39.164632: step 387, loss 0.421602, acc 0.804688\n",
            "2019-10-02T16:30:39.730587: step 388, loss 0.351851, acc 0.828125\n",
            "2019-10-02T16:30:40.289958: step 389, loss 0.46056, acc 0.765625\n",
            "2019-10-02T16:30:40.856028: step 390, loss 0.474973, acc 0.773438\n",
            "2019-10-02T16:30:41.374652: step 391, loss 0.443785, acc 0.78125\n",
            "2019-10-02T16:30:41.918775: step 392, loss 0.531305, acc 0.78125\n",
            "2019-10-02T16:30:42.449152: step 393, loss 0.381539, acc 0.820312\n",
            "2019-10-02T16:30:43.007993: step 394, loss 0.391729, acc 0.820312\n",
            "2019-10-02T16:30:43.546341: step 395, loss 0.458895, acc 0.765625\n",
            "2019-10-02T16:30:44.124449: step 396, loss 0.456703, acc 0.789062\n",
            "2019-10-02T16:30:44.687951: step 397, loss 0.437033, acc 0.796875\n",
            "2019-10-02T16:30:45.258608: step 398, loss 0.448428, acc 0.78125\n",
            "2019-10-02T16:30:45.811364: step 399, loss 0.492529, acc 0.796875\n",
            "2019-10-02T16:30:46.337421: step 400, loss 0.408324, acc 0.789062\n",
            "\n",
            "Evaluation:\n",
            "2019-10-02T16:30:46.377784: step 400, loss 0.590524, acc 0.708\n",
            "\n",
            "2019-10-02T16:30:46.940467: step 401, loss 0.48203, acc 0.75\n",
            "2019-10-02T16:30:47.481269: step 402, loss 0.378923, acc 0.828125\n",
            "2019-10-02T16:30:48.039665: step 403, loss 0.431362, acc 0.820312\n",
            "2019-10-02T16:30:48.573612: step 404, loss 0.416448, acc 0.820312\n",
            "2019-10-02T16:30:49.146835: step 405, loss 0.477244, acc 0.773438\n",
            "2019-10-02T16:30:49.695139: step 406, loss 0.41771, acc 0.757812\n",
            "2019-10-02T16:30:50.261819: step 407, loss 0.458034, acc 0.78125\n",
            "2019-10-02T16:30:50.799425: step 408, loss 0.371894, acc 0.828125\n",
            "2019-10-02T16:30:51.352626: step 409, loss 0.491926, acc 0.78125\n",
            "2019-10-02T16:30:51.916795: step 410, loss 0.531624, acc 0.742188\n",
            "2019-10-02T16:30:52.463937: step 411, loss 0.449689, acc 0.789062\n",
            "2019-10-02T16:30:53.022946: step 412, loss 0.480233, acc 0.75\n",
            "2019-10-02T16:30:53.578127: step 413, loss 0.481963, acc 0.757812\n",
            "2019-10-02T16:30:54.134711: step 414, loss 0.450129, acc 0.757812\n",
            "2019-10-02T16:30:54.677025: step 415, loss 0.476278, acc 0.78125\n",
            "2019-10-02T16:30:55.225454: step 416, loss 0.375409, acc 0.84375\n",
            "2019-10-02T16:30:55.761987: step 417, loss 0.427335, acc 0.796875\n",
            "2019-10-02T16:30:56.313120: step 418, loss 0.478133, acc 0.710938\n",
            "2019-10-02T16:30:56.858888: step 419, loss 0.4315, acc 0.742188\n",
            "2019-10-02T16:30:57.409439: step 420, loss 0.384855, acc 0.8125\n",
            "2019-10-02T16:30:57.944318: step 421, loss 0.530694, acc 0.773438\n",
            "2019-10-02T16:30:58.469170: step 422, loss 0.547202, acc 0.757812\n",
            "2019-10-02T16:30:59.011327: step 423, loss 0.541637, acc 0.75\n",
            "2019-10-02T16:30:59.567340: step 424, loss 0.462905, acc 0.804688\n",
            "2019-10-02T16:31:00.100309: step 425, loss 0.415599, acc 0.796875\n",
            "2019-10-02T16:31:00.647855: step 426, loss 0.410763, acc 0.775\n",
            "2019-10-02T16:31:01.213534: step 427, loss 0.454176, acc 0.8125\n",
            "2019-10-02T16:31:01.735942: step 428, loss 0.398066, acc 0.828125\n",
            "2019-10-02T16:31:02.279307: step 429, loss 0.391922, acc 0.84375\n",
            "2019-10-02T16:31:02.827477: step 430, loss 0.454607, acc 0.789062\n",
            "2019-10-02T16:31:03.366949: step 431, loss 0.388664, acc 0.828125\n",
            "2019-10-02T16:31:03.909536: step 432, loss 0.3747, acc 0.84375\n",
            "2019-10-02T16:31:04.473527: step 433, loss 0.399959, acc 0.8125\n",
            "2019-10-02T16:31:05.023989: step 434, loss 0.399291, acc 0.804688\n",
            "2019-10-02T16:31:05.578900: step 435, loss 0.313748, acc 0.890625\n",
            "2019-10-02T16:31:06.118423: step 436, loss 0.370076, acc 0.820312\n",
            "2019-10-02T16:31:06.660338: step 437, loss 0.406304, acc 0.796875\n",
            "2019-10-02T16:31:07.196794: step 438, loss 0.343649, acc 0.859375\n",
            "2019-10-02T16:31:07.754811: step 439, loss 0.326637, acc 0.898438\n",
            "2019-10-02T16:31:08.303853: step 440, loss 0.461842, acc 0.773438\n",
            "2019-10-02T16:31:08.835979: step 441, loss 0.359779, acc 0.828125\n",
            "2019-10-02T16:31:09.390221: step 442, loss 0.338046, acc 0.867188\n",
            "2019-10-02T16:31:09.935140: step 443, loss 0.359069, acc 0.828125\n",
            "2019-10-02T16:31:10.496133: step 444, loss 0.353323, acc 0.8125\n",
            "2019-10-02T16:31:11.003711: step 445, loss 0.370219, acc 0.84375\n",
            "2019-10-02T16:31:11.531278: step 446, loss 0.380103, acc 0.851562\n",
            "2019-10-02T16:31:12.084594: step 447, loss 0.51732, acc 0.710938\n",
            "2019-10-02T16:31:12.648167: step 448, loss 0.344492, acc 0.835938\n",
            "2019-10-02T16:31:13.211631: step 449, loss 0.378032, acc 0.835938\n",
            "2019-10-02T16:31:13.769629: step 450, loss 0.353928, acc 0.84375\n",
            "2019-10-02T16:31:14.324470: step 451, loss 0.47092, acc 0.789062\n",
            "2019-10-02T16:31:14.884494: step 452, loss 0.367725, acc 0.851562\n",
            "2019-10-02T16:31:15.414952: step 453, loss 0.395019, acc 0.804688\n",
            "2019-10-02T16:31:15.967512: step 454, loss 0.356674, acc 0.84375\n",
            "2019-10-02T16:31:16.517009: step 455, loss 0.348264, acc 0.851562\n",
            "2019-10-02T16:31:17.048233: step 456, loss 0.44746, acc 0.796875\n",
            "2019-10-02T16:31:17.590670: step 457, loss 0.457374, acc 0.796875\n",
            "2019-10-02T16:31:18.160416: step 458, loss 0.365569, acc 0.867188\n",
            "2019-10-02T16:31:18.713523: step 459, loss 0.346319, acc 0.84375\n",
            "2019-10-02T16:31:19.260532: step 460, loss 0.449421, acc 0.765625\n",
            "2019-10-02T16:31:19.820241: step 461, loss 0.409831, acc 0.796875\n",
            "2019-10-02T16:31:20.353258: step 462, loss 0.50703, acc 0.773438\n",
            "2019-10-02T16:31:20.894264: step 463, loss 0.429013, acc 0.804688\n",
            "2019-10-02T16:31:21.457288: step 464, loss 0.347158, acc 0.820312\n",
            "2019-10-02T16:31:22.003920: step 465, loss 0.392789, acc 0.84375\n",
            "2019-10-02T16:31:22.559818: step 466, loss 0.381419, acc 0.84375\n",
            "2019-10-02T16:31:23.121313: step 467, loss 0.321114, acc 0.867188\n",
            "2019-10-02T16:31:23.649433: step 468, loss 0.323617, acc 0.875\n",
            "2019-10-02T16:31:24.213950: step 469, loss 0.411111, acc 0.789062\n",
            "2019-10-02T16:31:24.762929: step 470, loss 0.358941, acc 0.835938\n",
            "2019-10-02T16:31:25.309662: step 471, loss 0.48115, acc 0.75\n",
            "2019-10-02T16:31:25.858456: step 472, loss 0.417306, acc 0.789062\n",
            "2019-10-02T16:31:26.400028: step 473, loss 0.357499, acc 0.867188\n",
            "2019-10-02T16:31:26.931357: step 474, loss 0.426564, acc 0.8125\n",
            "2019-10-02T16:31:27.497430: step 475, loss 0.335646, acc 0.84375\n",
            "2019-10-02T16:31:28.032878: step 476, loss 0.386549, acc 0.851562\n",
            "2019-10-02T16:31:28.559247: step 477, loss 0.413328, acc 0.78125\n",
            "2019-10-02T16:31:29.103107: step 478, loss 0.416958, acc 0.820312\n",
            "2019-10-02T16:31:29.638176: step 479, loss 0.418323, acc 0.820312\n",
            "2019-10-02T16:31:30.168257: step 480, loss 0.324743, acc 0.882812\n",
            "2019-10-02T16:31:30.703368: step 481, loss 0.412443, acc 0.851562\n",
            "2019-10-02T16:31:31.248531: step 482, loss 0.38107, acc 0.828125\n",
            "2019-10-02T16:31:31.802291: step 483, loss 0.458541, acc 0.773438\n",
            "2019-10-02T16:31:32.353297: step 484, loss 0.38363, acc 0.8125\n",
            "2019-10-02T16:31:32.908237: step 485, loss 0.468832, acc 0.765625\n",
            "2019-10-02T16:31:33.445666: step 486, loss 0.383799, acc 0.828125\n",
            "2019-10-02T16:31:33.977444: step 487, loss 0.332276, acc 0.859375\n",
            "2019-10-02T16:31:34.541388: step 488, loss 0.337442, acc 0.851562\n",
            "2019-10-02T16:31:35.096051: step 489, loss 0.443843, acc 0.828125\n",
            "2019-10-02T16:31:35.660040: step 490, loss 0.391404, acc 0.820312\n",
            "2019-10-02T16:31:36.188632: step 491, loss 0.36897, acc 0.84375\n",
            "2019-10-02T16:31:36.749132: step 492, loss 0.377883, acc 0.859375\n",
            "2019-10-02T16:31:37.320384: step 493, loss 0.369656, acc 0.820312\n",
            "2019-10-02T16:31:37.860857: step 494, loss 0.394358, acc 0.820312\n",
            "2019-10-02T16:31:38.418246: step 495, loss 0.389096, acc 0.851562\n",
            "2019-10-02T16:31:38.949643: step 496, loss 0.437159, acc 0.820312\n",
            "2019-10-02T16:31:39.510190: step 497, loss 0.50068, acc 0.75\n",
            "2019-10-02T16:31:40.059598: step 498, loss 0.357615, acc 0.851562\n",
            "2019-10-02T16:31:40.620096: step 499, loss 0.346983, acc 0.828125\n",
            "2019-10-02T16:31:41.165974: step 500, loss 0.339022, acc 0.835938\n",
            "\n",
            "Evaluation:\n",
            "2019-10-02T16:31:41.208194: step 500, loss 0.604931, acc 0.704\n",
            "\n",
            "2019-10-02T16:31:41.774505: step 501, loss 0.40033, acc 0.859375\n",
            "2019-10-02T16:31:42.300946: step 502, loss 0.346453, acc 0.851562\n",
            "2019-10-02T16:31:42.862454: step 503, loss 0.316745, acc 0.835938\n",
            "2019-10-02T16:31:43.403211: step 504, loss 0.269209, acc 0.90625\n",
            "2019-10-02T16:31:43.977471: step 505, loss 0.316333, acc 0.875\n",
            "2019-10-02T16:31:44.510363: step 506, loss 0.317656, acc 0.875\n",
            "2019-10-02T16:31:45.060602: step 507, loss 0.443197, acc 0.835938\n",
            "2019-10-02T16:31:45.623890: step 508, loss 0.317904, acc 0.867188\n",
            "2019-10-02T16:31:46.164096: step 509, loss 0.29044, acc 0.890625\n",
            "2019-10-02T16:31:46.721155: step 510, loss 0.349798, acc 0.84375\n",
            "2019-10-02T16:31:47.285210: step 511, loss 0.337219, acc 0.875\n",
            "2019-10-02T16:31:47.838403: step 512, loss 0.354714, acc 0.859375\n",
            "2019-10-02T16:31:48.375671: step 513, loss 0.3407, acc 0.859375\n",
            "2019-10-02T16:31:48.913200: step 514, loss 0.372496, acc 0.84375\n",
            "2019-10-02T16:31:49.480892: step 515, loss 0.302896, acc 0.859375\n",
            "2019-10-02T16:31:50.051275: step 516, loss 0.339412, acc 0.851562\n",
            "2019-10-02T16:31:50.606204: step 517, loss 0.293734, acc 0.867188\n",
            "2019-10-02T16:31:51.176263: step 518, loss 0.370353, acc 0.890625\n",
            "2019-10-02T16:31:51.719235: step 519, loss 0.275719, acc 0.929688\n",
            "2019-10-02T16:31:52.281242: step 520, loss 0.407059, acc 0.8125\n",
            "2019-10-02T16:31:52.858150: step 521, loss 0.405797, acc 0.796875\n",
            "2019-10-02T16:31:53.411182: step 522, loss 0.337692, acc 0.867188\n",
            "2019-10-02T16:31:53.948769: step 523, loss 0.401401, acc 0.8125\n",
            "2019-10-02T16:31:54.483963: step 524, loss 0.386481, acc 0.804688\n",
            "2019-10-02T16:31:55.041590: step 525, loss 0.337426, acc 0.851562\n",
            "2019-10-02T16:31:55.576114: step 526, loss 0.278561, acc 0.882812\n",
            "2019-10-02T16:31:56.102910: step 527, loss 0.26652, acc 0.898438\n",
            "2019-10-02T16:31:56.660775: step 528, loss 0.432543, acc 0.828125\n",
            "2019-10-02T16:31:57.227569: step 529, loss 0.338006, acc 0.835938\n",
            "2019-10-02T16:31:57.795278: step 530, loss 0.293351, acc 0.882812\n",
            "2019-10-02T16:31:58.363179: step 531, loss 0.345638, acc 0.859375\n",
            "2019-10-02T16:31:58.895773: step 532, loss 0.443454, acc 0.78125\n",
            "2019-10-02T16:31:59.449628: step 533, loss 0.291212, acc 0.875\n",
            "2019-10-02T16:32:00.002263: step 534, loss 0.315548, acc 0.890625\n",
            "2019-10-02T16:32:00.539360: step 535, loss 0.357572, acc 0.828125\n",
            "2019-10-02T16:32:01.085053: step 536, loss 0.288808, acc 0.90625\n",
            "2019-10-02T16:32:01.606998: step 537, loss 0.322069, acc 0.84375\n",
            "2019-10-02T16:32:02.160766: step 538, loss 0.295256, acc 0.867188\n",
            "2019-10-02T16:32:02.714857: step 539, loss 0.392129, acc 0.820312\n",
            "2019-10-02T16:32:03.274040: step 540, loss 0.404708, acc 0.820312\n",
            "2019-10-02T16:32:03.806158: step 541, loss 0.218286, acc 0.921875\n",
            "2019-10-02T16:32:04.353246: step 542, loss 0.367093, acc 0.851562\n",
            "2019-10-02T16:32:04.897717: step 543, loss 0.322986, acc 0.867188\n",
            "2019-10-02T16:32:05.446419: step 544, loss 0.339265, acc 0.851562\n",
            "2019-10-02T16:32:05.985822: step 545, loss 0.414343, acc 0.789062\n",
            "2019-10-02T16:32:06.530865: step 546, loss 0.250997, acc 0.90625\n",
            "2019-10-02T16:32:07.073500: step 547, loss 0.272394, acc 0.882812\n",
            "2019-10-02T16:32:07.594341: step 548, loss 0.27352, acc 0.890625\n",
            "2019-10-02T16:32:08.127106: step 549, loss 0.363625, acc 0.835938\n",
            "2019-10-02T16:32:08.680511: step 550, loss 0.331553, acc 0.820312\n",
            "2019-10-02T16:32:09.215814: step 551, loss 0.372459, acc 0.875\n",
            "2019-10-02T16:32:09.766218: step 552, loss 0.379724, acc 0.828125\n",
            "2019-10-02T16:32:10.314545: step 553, loss 0.454057, acc 0.804688\n",
            "2019-10-02T16:32:10.848265: step 554, loss 0.296557, acc 0.890625\n",
            "2019-10-02T16:32:11.389777: step 555, loss 0.359848, acc 0.84375\n",
            "2019-10-02T16:32:11.927624: step 556, loss 0.511056, acc 0.8125\n",
            "2019-10-02T16:32:12.478106: step 557, loss 0.431236, acc 0.789062\n",
            "2019-10-02T16:32:13.059494: step 558, loss 0.31142, acc 0.890625\n",
            "2019-10-02T16:32:13.617896: step 559, loss 0.301408, acc 0.859375\n",
            "2019-10-02T16:32:14.200492: step 560, loss 0.359684, acc 0.84375\n",
            "2019-10-02T16:32:14.769744: step 561, loss 0.321961, acc 0.898438\n",
            "2019-10-02T16:32:15.313327: step 562, loss 0.32657, acc 0.867188\n",
            "2019-10-02T16:32:15.856559: step 563, loss 0.27906, acc 0.898438\n",
            "2019-10-02T16:32:16.402947: step 564, loss 0.374282, acc 0.835938\n",
            "2019-10-02T16:32:16.936706: step 565, loss 0.331796, acc 0.835938\n",
            "2019-10-02T16:32:17.486266: step 566, loss 0.318052, acc 0.867188\n",
            "2019-10-02T16:32:18.024243: step 567, loss 0.353097, acc 0.835938\n",
            "2019-10-02T16:32:18.550476: step 568, loss 0.231302, acc 0.925\n",
            "2019-10-02T16:32:19.101706: step 569, loss 0.278148, acc 0.898438\n",
            "2019-10-02T16:32:19.653169: step 570, loss 0.314511, acc 0.875\n",
            "2019-10-02T16:32:20.219402: step 571, loss 0.331064, acc 0.859375\n",
            "2019-10-02T16:32:20.770084: step 572, loss 0.414488, acc 0.828125\n",
            "2019-10-02T16:32:21.318157: step 573, loss 0.272495, acc 0.875\n",
            "2019-10-02T16:32:21.871916: step 574, loss 0.276343, acc 0.890625\n",
            "2019-10-02T16:32:22.424899: step 575, loss 0.34012, acc 0.828125\n",
            "2019-10-02T16:32:22.982299: step 576, loss 0.326663, acc 0.859375\n",
            "2019-10-02T16:32:23.539680: step 577, loss 0.268939, acc 0.890625\n",
            "2019-10-02T16:32:24.104310: step 578, loss 0.281963, acc 0.898438\n",
            "2019-10-02T16:32:24.680989: step 579, loss 0.327144, acc 0.851562\n",
            "2019-10-02T16:32:25.225504: step 580, loss 0.286651, acc 0.867188\n",
            "2019-10-02T16:32:25.772019: step 581, loss 0.272881, acc 0.882812\n",
            "2019-10-02T16:32:26.335840: step 582, loss 0.279536, acc 0.882812\n",
            "2019-10-02T16:32:26.893097: step 583, loss 0.446123, acc 0.78125\n",
            "2019-10-02T16:32:27.426326: step 584, loss 0.217922, acc 0.90625\n",
            "2019-10-02T16:32:27.980232: step 585, loss 0.343294, acc 0.882812\n",
            "2019-10-02T16:32:28.522346: step 586, loss 0.277074, acc 0.921875\n",
            "2019-10-02T16:32:29.078722: step 587, loss 0.309592, acc 0.859375\n",
            "2019-10-02T16:32:29.623311: step 588, loss 0.26208, acc 0.851562\n",
            "2019-10-02T16:32:30.182498: step 589, loss 0.238258, acc 0.882812\n",
            "2019-10-02T16:32:30.732825: step 590, loss 0.27138, acc 0.882812\n",
            "2019-10-02T16:32:31.291090: step 591, loss 0.275899, acc 0.875\n",
            "2019-10-02T16:32:31.831210: step 592, loss 0.18097, acc 0.945312\n",
            "2019-10-02T16:32:32.378458: step 593, loss 0.247902, acc 0.90625\n",
            "2019-10-02T16:32:32.913050: step 594, loss 0.237791, acc 0.90625\n",
            "2019-10-02T16:32:33.453872: step 595, loss 0.246116, acc 0.921875\n",
            "2019-10-02T16:32:33.984797: step 596, loss 0.248341, acc 0.875\n",
            "2019-10-02T16:32:34.527892: step 597, loss 0.252738, acc 0.898438\n",
            "2019-10-02T16:32:35.067667: step 598, loss 0.241313, acc 0.890625\n",
            "2019-10-02T16:32:35.660127: step 599, loss 0.326758, acc 0.851562\n",
            "2019-10-02T16:32:36.200862: step 600, loss 0.259249, acc 0.851562\n",
            "\n",
            "Evaluation:\n",
            "2019-10-02T16:32:36.239337: step 600, loss 0.742137, acc 0.713\n",
            "\n",
            "2019-10-02T16:32:36.788903: step 601, loss 0.282895, acc 0.867188\n",
            "2019-10-02T16:32:37.348348: step 602, loss 0.253455, acc 0.914062\n",
            "2019-10-02T16:32:37.879832: step 603, loss 0.250091, acc 0.875\n",
            "2019-10-02T16:32:38.417382: step 604, loss 0.279592, acc 0.882812\n",
            "2019-10-02T16:32:38.960335: step 605, loss 0.213558, acc 0.921875\n",
            "2019-10-02T16:32:39.540286: step 606, loss 0.308652, acc 0.898438\n",
            "2019-10-02T16:32:40.087449: step 607, loss 0.172608, acc 0.9375\n",
            "2019-10-02T16:32:40.639901: step 608, loss 0.344015, acc 0.851562\n",
            "2019-10-02T16:32:41.217642: step 609, loss 0.226604, acc 0.90625\n",
            "2019-10-02T16:32:41.748793: step 610, loss 0.187183, acc 0.921875\n",
            "2019-10-02T16:32:42.289867: step 611, loss 0.34891, acc 0.828125\n",
            "2019-10-02T16:32:42.834306: step 612, loss 0.313897, acc 0.882812\n",
            "2019-10-02T16:32:43.367875: step 613, loss 0.369444, acc 0.875\n",
            "2019-10-02T16:32:43.931795: step 614, loss 0.26639, acc 0.898438\n",
            "2019-10-02T16:32:44.485488: step 615, loss 0.382118, acc 0.820312\n",
            "2019-10-02T16:32:45.035199: step 616, loss 0.216504, acc 0.90625\n",
            "2019-10-02T16:32:45.591075: step 617, loss 0.32438, acc 0.882812\n",
            "2019-10-02T16:32:46.141777: step 618, loss 0.350121, acc 0.84375\n",
            "2019-10-02T16:32:46.691686: step 619, loss 0.2878, acc 0.898438\n",
            "2019-10-02T16:32:47.238797: step 620, loss 0.330698, acc 0.867188\n",
            "2019-10-02T16:32:47.785448: step 621, loss 0.364548, acc 0.828125\n",
            "2019-10-02T16:32:48.329396: step 622, loss 0.251562, acc 0.921875\n",
            "2019-10-02T16:32:48.885370: step 623, loss 0.32813, acc 0.875\n",
            "2019-10-02T16:32:49.440063: step 624, loss 0.244161, acc 0.898438\n",
            "2019-10-02T16:32:49.981263: step 625, loss 0.319757, acc 0.859375\n",
            "2019-10-02T16:32:50.513841: step 626, loss 0.324803, acc 0.859375\n",
            "2019-10-02T16:32:51.050295: step 627, loss 0.351581, acc 0.84375\n",
            "2019-10-02T16:32:51.595329: step 628, loss 0.284584, acc 0.898438\n",
            "2019-10-02T16:32:52.140354: step 629, loss 0.287784, acc 0.867188\n",
            "2019-10-02T16:32:52.706558: step 630, loss 0.241603, acc 0.90625\n",
            "2019-10-02T16:32:53.265842: step 631, loss 0.291777, acc 0.867188\n",
            "2019-10-02T16:32:53.806032: step 632, loss 0.252488, acc 0.898438\n",
            "2019-10-02T16:32:54.352464: step 633, loss 0.441684, acc 0.828125\n",
            "2019-10-02T16:32:54.887085: step 634, loss 0.319814, acc 0.867188\n",
            "2019-10-02T16:32:55.447856: step 635, loss 0.323884, acc 0.859375\n",
            "2019-10-02T16:32:56.007138: step 636, loss 0.273763, acc 0.890625\n",
            "2019-10-02T16:32:56.556487: step 637, loss 0.389904, acc 0.867188\n",
            "2019-10-02T16:32:57.121367: step 638, loss 0.373284, acc 0.851562\n",
            "2019-10-02T16:32:57.637650: step 639, loss 0.314492, acc 0.9\n",
            "2019-10-02T16:32:58.199267: step 640, loss 0.296151, acc 0.859375\n",
            "2019-10-02T16:32:58.743818: step 641, loss 0.231166, acc 0.914062\n",
            "2019-10-02T16:32:59.315907: step 642, loss 0.253785, acc 0.890625\n",
            "2019-10-02T16:32:59.857288: step 643, loss 0.271246, acc 0.875\n",
            "2019-10-02T16:33:00.403214: step 644, loss 0.276427, acc 0.898438\n",
            "2019-10-02T16:33:00.919439: step 645, loss 0.214036, acc 0.914062\n",
            "2019-10-02T16:33:01.470008: step 646, loss 0.26747, acc 0.875\n",
            "2019-10-02T16:33:02.012970: step 647, loss 0.253702, acc 0.90625\n",
            "2019-10-02T16:33:02.559870: step 648, loss 0.191056, acc 0.914062\n",
            "2019-10-02T16:33:03.113853: step 649, loss 0.221843, acc 0.921875\n",
            "2019-10-02T16:33:03.651250: step 650, loss 0.227297, acc 0.914062\n",
            "2019-10-02T16:33:04.200055: step 651, loss 0.207292, acc 0.90625\n",
            "2019-10-02T16:33:04.723018: step 652, loss 0.285354, acc 0.90625\n",
            "2019-10-02T16:33:05.266783: step 653, loss 0.271192, acc 0.890625\n",
            "2019-10-02T16:33:05.777865: step 654, loss 0.269503, acc 0.898438\n",
            "2019-10-02T16:33:06.316883: step 655, loss 0.361317, acc 0.867188\n",
            "2019-10-02T16:33:06.857378: step 656, loss 0.254913, acc 0.914062\n",
            "2019-10-02T16:33:07.410001: step 657, loss 0.250906, acc 0.898438\n",
            "2019-10-02T16:33:07.941592: step 658, loss 0.273488, acc 0.851562\n",
            "2019-10-02T16:33:08.472977: step 659, loss 0.215832, acc 0.890625\n",
            "2019-10-02T16:33:08.987996: step 660, loss 0.235095, acc 0.90625\n",
            "2019-10-02T16:33:09.542523: step 661, loss 0.313195, acc 0.875\n",
            "2019-10-02T16:33:10.071139: step 662, loss 0.178885, acc 0.929688\n",
            "2019-10-02T16:33:10.608383: step 663, loss 0.277462, acc 0.890625\n",
            "2019-10-02T16:33:11.142860: step 664, loss 0.210369, acc 0.914062\n",
            "2019-10-02T16:33:11.699201: step 665, loss 0.219143, acc 0.921875\n",
            "2019-10-02T16:33:12.240252: step 666, loss 0.277842, acc 0.914062\n",
            "2019-10-02T16:33:12.803263: step 667, loss 0.270103, acc 0.890625\n",
            "2019-10-02T16:33:13.337639: step 668, loss 0.22654, acc 0.890625\n",
            "2019-10-02T16:33:13.899847: step 669, loss 0.261, acc 0.898438\n",
            "2019-10-02T16:33:14.443788: step 670, loss 0.315031, acc 0.859375\n",
            "2019-10-02T16:33:15.003781: step 671, loss 0.280312, acc 0.859375\n",
            "2019-10-02T16:33:15.530226: step 672, loss 0.219229, acc 0.921875\n",
            "2019-10-02T16:33:16.070037: step 673, loss 0.322882, acc 0.882812\n",
            "2019-10-02T16:33:16.619682: step 674, loss 0.218716, acc 0.9375\n",
            "2019-10-02T16:33:17.182923: step 675, loss 0.309233, acc 0.867188\n",
            "2019-10-02T16:33:17.746880: step 676, loss 0.269857, acc 0.898438\n",
            "2019-10-02T16:33:18.293444: step 677, loss 0.228788, acc 0.890625\n",
            "2019-10-02T16:33:18.854425: step 678, loss 0.200096, acc 0.914062\n",
            "2019-10-02T16:33:19.395177: step 679, loss 0.197644, acc 0.921875\n",
            "2019-10-02T16:33:19.944027: step 680, loss 0.189054, acc 0.945312\n",
            "2019-10-02T16:33:20.495646: step 681, loss 0.147621, acc 0.953125\n",
            "2019-10-02T16:33:21.048136: step 682, loss 0.248023, acc 0.90625\n",
            "2019-10-02T16:33:21.595915: step 683, loss 0.273671, acc 0.90625\n",
            "2019-10-02T16:33:22.163467: step 684, loss 0.266915, acc 0.875\n",
            "2019-10-02T16:33:22.717803: step 685, loss 0.301362, acc 0.867188\n",
            "2019-10-02T16:33:23.286848: step 686, loss 0.23947, acc 0.882812\n",
            "2019-10-02T16:33:23.857154: step 687, loss 0.198157, acc 0.914062\n",
            "2019-10-02T16:33:24.420726: step 688, loss 0.232297, acc 0.867188\n",
            "2019-10-02T16:33:24.976998: step 689, loss 0.355531, acc 0.859375\n",
            "2019-10-02T16:33:25.516259: step 690, loss 0.227989, acc 0.90625\n",
            "2019-10-02T16:33:26.069489: step 691, loss 0.286381, acc 0.882812\n",
            "2019-10-02T16:33:26.623176: step 692, loss 0.211813, acc 0.929688\n",
            "2019-10-02T16:33:27.181685: step 693, loss 0.201859, acc 0.898438\n",
            "2019-10-02T16:33:27.712714: step 694, loss 0.250996, acc 0.890625\n",
            "2019-10-02T16:33:28.277634: step 695, loss 0.167504, acc 0.945312\n",
            "2019-10-02T16:33:28.828819: step 696, loss 0.20464, acc 0.890625\n",
            "2019-10-02T16:33:29.403913: step 697, loss 0.213007, acc 0.898438\n",
            "2019-10-02T16:33:29.972710: step 698, loss 0.204202, acc 0.9375\n",
            "2019-10-02T16:33:30.512857: step 699, loss 0.224431, acc 0.882812\n",
            "2019-10-02T16:33:31.062616: step 700, loss 0.317659, acc 0.851562\n",
            "\n",
            "Evaluation:\n",
            "2019-10-02T16:33:31.105451: step 700, loss 0.788514, acc 0.713\n",
            "\n",
            "2019-10-02T16:33:31.650388: step 701, loss 0.173835, acc 0.914062\n",
            "2019-10-02T16:33:32.210426: step 702, loss 0.29489, acc 0.851562\n",
            "2019-10-02T16:33:32.744760: step 703, loss 0.205142, acc 0.914062\n",
            "2019-10-02T16:33:33.279306: step 704, loss 0.271374, acc 0.859375\n",
            "2019-10-02T16:33:33.815330: step 705, loss 0.18061, acc 0.9375\n",
            "2019-10-02T16:33:34.376960: step 706, loss 0.25331, acc 0.898438\n",
            "2019-10-02T16:33:34.937931: step 707, loss 0.294025, acc 0.90625\n",
            "2019-10-02T16:33:35.480096: step 708, loss 0.272744, acc 0.890625\n",
            "2019-10-02T16:33:36.005942: step 709, loss 0.218171, acc 0.890625\n",
            "2019-10-02T16:33:36.558814: step 710, loss 0.264057, acc 0.95\n",
            "2019-10-02T16:33:36.599642: step 710, loss 0.750126, acc 0.718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEDRdbNC-txI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}